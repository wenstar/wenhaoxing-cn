```toml
title = "FaceNet笔记"
slug = "FaceNet-notes"
desc = "FaceNet笔记"
date = "2018-11-27 21:35:35"
update_date = "2018-12-22 17:45:35"
author = "皓星"
draft = false
tags = ["技术"]
```

FaceNet 论文，思路和一点测试
----
FaceNet在[GitHub](https://github.com/davidsandberg/facenet)上的实现，包括训练好的网络  
论文是：FaceNet: A Unified Embedding for Face Recognition and Clustering

最近参与实验室的工作，比较详细的了解了FaceNet，做一点笔记。  

####简介
FaceNet 将一张人脸图片映射到欧几里得空间，得到一个坐标，两张人脸图片坐标的欧几里得距离越近，则说明其身份近似，在论文和实际测试中，可行的距离阈值为1.1, 即距离小于阈值的人脸是同一个人的。
在 2018/11/27 的 GitHub 中，输入为160*160*3的图片，输出为512维坐标，提供的训练好的网络采用了ResNet。

这里需要注意，在论文中采用的网络结构和输出坐标维度都不相同。
  
结合论文中作者的描述，可以猜测：更高的维度可以带来更好的精度，代价则是更加难以训练，而借助于网络结构的进步，让我们更好的训练整个网络，从而通过512维坐标得到更高精度的结果。
FaceNet 的主要特点在于使用了Triplet Loss训练网络，而Triplet Loss我翻译为三重loss，指的是在训练时将Anchor和Positive的距离减少，将Anchor和Negative的距离增加。
这个loss指向的就是最终同一个人脸的坐标会聚集在一起，并且会和其他人的人脸坐标相分割。论文中提到的他人的工作，包括我的直接想法，是将同一个人的许多脸映射到同一个点上，相比之下，聚集而不是同一个点的想法应该更符合直观感受，因该更利于训练和得到更高精度的结果。

另一方面，FaceNet的输入数据就是人脸图片本身，不需要做任何对齐等操作就直接喂给神经网络，从各种经验看，如果这样的方法能work，通常都意味着更好的精度。一个推论是，在良好训练后，学习得到的特征，比人类手工选择的特征更好。之前讨论过，ResNet是一个通用逼近器，通用逼近到的“上帝算法”中通常不包含人手进行的各种操作。（也许包含类似的部分操作）

###Triplet Loss
FaceNet 的难点在于，如何高效率的训练网络，表现为如何选择Anchor，Positive和Negative。

- 如果对所有的任意三元组都进行训练，很显然有很多三元组天然满足Positive更近和Negative更远的要求，对训练不做出贡献，而且数量也太多了。

- 为了能更快的训练，可以选择最远的Positive和最近的Negative，称之为hard positive/negative，但是这又会导致poor training，因为最hard的选择通常很可能是误标注数据和图像/人脸特征很差的数据。

- 作者采取的方案是在每一个mini-batch中选择最hard的数据（不是真的最hard，还有其他trade-off）

- 为了确保每一个mini-batch中都有positive距离的有意义的表示，每一个mini-batch中都有每个id的约40张脸，和randomly sampled negative faces，一个mini-batch是1,800个例子（数据来自论文，和当前GitHub的实现可能不同）

- 作者使用了所有的positive距离，和hard的negative距离。最hard的negative会过早的导致局部最优，所以作者选择了semi-hard的negative，即是比positive更远，但是又不够远，lie inside the margin。

###人脸检测
一次完整的人脸检测首先使用 mtcnn 将人脸从原始图片中框出来，然后将框内的人脸交给facenet处理，得到一个坐标，然后将坐标与已有数据计算欧几里得距离，然后能判断该人脸的身份。本次项目试图加速这一流程。于是对facenet进行了推理阶段测速，不考虑训练，输入数据是预处理好的 mtcnn 输出，输出为坐标，不考虑对坐标的进一步处理。

###测速和优化
测速平台是新搭建的 2700x + 2080，CUDA9.0，tf1.11，一次输入（一个batch）512张图片时，处理需要约1.2s到1.4s，即0.0025s一张图片，速度还是很快的。测试了VGGFACE2的test数据集约17万张图片，速度稳定。

简单观察，不考虑深度优化，也还有提速空间。由于没有使用tf.Data，每次喂数据都会空闲gpu；而且输入的是图片，还需要将rbg数据映射为小数，如果把这一步加入预处理，直接输入处理好的矩阵，应该能更快一点。

另外限制batch size的是显存大小，如果合理优化一下各种数据结构，进一步增大batch size，速度也能更快。

题外话，仅就这一网络而言，2080相比于1080没有任何速度优势。也许需要采用更新的CUDA才能表现出来进步？心疼钱。
​
###后续进展
当时还没有支持CUDA10的tensorflow预编译版本，所以无法体现2080的速度优势。
后来自己在Ubuntu上编译了一个基于CUDA10的tensorflow，实际测试速度相比1080可以有35%以上的提高。

这里的速度应该理解为**吞吐**和**延迟**两个方面。吞吐不仅与计算速度有关，还与batch size有关，而合适的batch size与显存大小紧密相关。
现在（20190422），基于CUDA10编译的tensorflow已经比较容易安装了。当时编译也是废了一番功夫。

为了能提高速度，也试用了TensorRT，还有TVM框架，过程比较曲折。

测试的平台包括ARM开发版，TX2，1080Ti和2080。下面的数据未使用TensorRT等。

其中2080和1080Ti的batch size做到64开始就不再能显著增加吞吐，即不能减少平均每张人脸耗时，认为吞吐达到阈值。

Batch Size小于阈值时增大它可以显著提高吞吐，大于阈值则不能显著提高吞吐，大于阈值一定范围内吞吐基本保持不变。

2080 8G显存，增大batch size到512，平均每张人脸耗时基本维持不变。
1080Ti 11G显存，增大batch size到1024，平均每张人脸耗时基本维持不变。

| 平台 | Tensorflow| Batch Size阈值 |平均每张人脸最小耗时(ms) |
|:---:|:---:|:---:|:---:|
|2080| TF1.12(自编译)，CUDA10.0|64|0.83|
|1080Ti| TF1.9，CUDA9.0|64|1.24|
|Jetson TX2| TF1.9，CUDA9.0|32|15.18|
|Hikey 960 (ARM)|TF1.4，Open CL|8|3943.39|
